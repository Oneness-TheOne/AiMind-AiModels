{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e7454f",
   "metadata": {},
   "source": [
    "# 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a6b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리가 정리되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. 모든 변수 참조 해제 시도\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "if 'inputs' in locals():\n",
    "    del inputs\n",
    "\n",
    "# 2. 파이썬 가비지 컬렉션 실행\n",
    "gc.collect()\n",
    "\n",
    "# 3. PyTorch 캐시 비우기\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(\"GPU 메모리가 정리되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff949db",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d9c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치(import)\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e652d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     14\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m     15\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     16\u001b[39m     bnb_4bit_compute_dtype=torch.float16,\n\u001b[32m     17\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     18\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 2. 모델 로드 (low_cpu_mem_usage 추가)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m model = \u001b[43mLlavaOnevisionForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msdpa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# CPU 메모리 사용 최적화\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m processor = AutoProcessor.from_pretrained(model_id)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 3. 이미지 크기 극단적 축소 (성공 여부 확인용)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4839\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4830\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4832\u001b[39m     (\n\u001b[32m   4833\u001b[39m         model,\n\u001b[32m   4834\u001b[39m         missing_keys,\n\u001b[32m   4835\u001b[39m         unexpected_keys,\n\u001b[32m   4836\u001b[39m         mismatched_keys,\n\u001b[32m   4837\u001b[39m         offload_index,\n\u001b[32m   4838\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4839\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4857\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4858\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5302\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5299\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5301\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5302\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5303\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5305\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:933\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:848\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    845\u001b[39m     _load_parameter_into_model(model, param_name, param.to(param_device))\n\u001b[32m    847\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[32m    852\u001b[39m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[32m    853\u001b[39m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[32m    854\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:250\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.create_quantized_param\u001b[39m\u001b[34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[39m\n\u001b[32m    247\u001b[39m         new_value = new_value.T\n\u001b[32m    249\u001b[39m     kwargs = old_value.\u001b[34m__dict__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     new_value = \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParams4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m module._parameters[tensor_name] = new_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:346\u001b[39m, in \u001b[36mParams4bit.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    343\u001b[39m device, dtype, non_blocking, _ = torch._C._nn._parse_to(*args, **kwargs)\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device.type != \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bnb_quantized:\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.quant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:301\u001b[39m, in \u001b[36mParams4bit._quantize\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_quantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[32m    300\u001b[39m     w = \u001b[38;5;28mself\u001b[39m.data.contiguous().to(device)\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     w_4bit, quant_state = \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize_4bit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompress_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompress_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquant_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquant_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquant_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = w_4bit\n\u001b[32m    309\u001b[39m     \u001b[38;5;28mself\u001b[39m.quant_state = quant_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\bitsandbytes\\functional.py:871\u001b[39m, in \u001b[36mquantize_4bit\u001b[39m\u001b[34m(A, absmax, out, blocksize, compress_statistics, quant_type, quant_storage)\u001b[39m\n\u001b[32m    862\u001b[39m input_shape = A.shape\n\u001b[32m    864\u001b[39m _out, _absmax = torch.ops.bitsandbytes.quantize_4bit.default(\n\u001b[32m    865\u001b[39m     A,\n\u001b[32m    866\u001b[39m     blocksize,\n\u001b[32m    867\u001b[39m     quant_type,\n\u001b[32m    868\u001b[39m     quant_storage,\n\u001b[32m    869\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m code = \u001b[43mget_4bit_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compress_statistics:\n\u001b[32m    874\u001b[39m     offset = _absmax.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\bitsandbytes\\functional.py:793\u001b[39m, in \u001b[36mget_4bit_type\u001b[39m\u001b[34m(typename, device, blocksize)\u001b[39m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTypename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    792\u001b[39m data = torch.tensor(data, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m data.div_(\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.max())\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m data.numel() == \u001b[32m16\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# [필수] 메모리 파편화 방지 및 관리 설정\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "clean_memory()\n",
    "\n",
    "model_id = \"NCSOFT/VARCO-VISION-2.0-1.7B-OCR\"\n",
    "\n",
    "# 1. 4비트 양자화 + CPU 오프로딩 준비\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# 2. 모델 로드 (low_cpu_mem_usage 추가)\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    low_cpu_mem_usage=True # CPU 메모리 사용 최적화\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 3. 이미지 크기 극단적 축소 (성공 여부 확인용)\n",
    "image_path = \"img1.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# 512나 768로 대폭 낮춥니다. (6GB에서는 1024도 벅찰 수 있습니다)\n",
    "target_size = 512 \n",
    "w, h = image.size\n",
    "if max(w, h) > target_size:\n",
    "    scale = target_size / max(w, h)\n",
    "    image = image.resize((int(w * scale), int(h * scale)), resample=Image.LANCZOS)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"<ocr>\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# 4. 입력 데이터 처리\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# 5. 추론 (더 쥐어짜기)\n",
    "clean_memory()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256, # 토큰 수를 확 줄여서 메모리 압박 해소\n",
    "        do_sample=False,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "# 6. 결과 출력\n",
    "output = processor.decode(generate_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "print(\"\\n=== OCR 결과 ===\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07205fb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ... (이전 추론 코드는 동일, max_new_tokens만 1024로 다시 늘려주세요)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     generate_ids = \u001b[43mmodel\u001b[49m.generate(\n\u001b[32m      7\u001b[39m         **inputs,\n\u001b[32m      8\u001b[39m         max_new_tokens=\u001b[32m1024\u001b[39m, \u001b[38;5;66;03m# 좌표값이 길어서 넉넉하게 줘야 끝까지 나옵니다.\u001b[39;00m\n\u001b[32m      9\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     10\u001b[39m         use_cache=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 결과 디코딩\u001b[39;00m\n\u001b[32m     14\u001b[39m full_output = processor.decode(generate_ids[\u001b[32m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(inputs.input_ids[\u001b[32m0\u001b[39m]):], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ... (이전 추론 코드는 동일, max_new_tokens만 1024로 다시 늘려주세요)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024, # 좌표값이 길어서 넉넉하게 줘야 끝까지 나옵니다.\n",
    "        do_sample=False,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "# 결과 디코딩\n",
    "full_output = processor.decode(generate_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "# 정규표현식을 사용해 숫자 좌표(0.123, 0.456 등)를 제거하는 함수\n",
    "def clean_ocr_text(text):\n",
    "    # 소수점 숫자가 포함된 좌표 패턴을 찾아 제거합니다.\n",
    "    cleaned = re.sub(r'\\d+\\.\\d+,\\s*\\d+\\.\\d+,\\s*\\d+\\.\\d+,\\s*\\d+\\.\\d+', '', text)\n",
    "    # 남은 숫자 조각들이나 불필요한 공백 정리\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "print(\"\\n=== 정제된 OCR 결과 ===\")\n",
    "print(clean_ocr_text(full_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4a65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
