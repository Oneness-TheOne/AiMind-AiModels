{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e7454f",
   "metadata": {},
   "source": [
    "# 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a6b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리가 정리되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. 모든 변수 참조 해제 시도\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "if 'inputs' in locals():\n",
    "    del inputs\n",
    "\n",
    "# 2. 파이썬 가비지 컬렉션 실행\n",
    "gc.collect()\n",
    "\n",
    "# 3. PyTorch 캐시 비우기\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(\"GPU 메모리가 정리되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff949db",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d9c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치(import)\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e652d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'qwen3'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     14\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m     15\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     16\u001b[39m     bnb_4bit_compute_dtype=torch.float16,\n\u001b[32m     17\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     18\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 2. 모델 로드 (low_cpu_mem_usage 추가)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m model = \u001b[43mLlavaOnevisionForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msdpa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# CPU 메모리 사용 최적화\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m processor = AutoProcessor.from_pretrained(model_id)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 3. 이미지 크기 극단적 축소 (성공 여부 확인용)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3625\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[32m   3624\u001b[39m     config_path = config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[32m-> \u001b[39m\u001b[32m3625\u001b[39m     config, model_kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3635\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3636\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3637\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3638\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3639\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3641\u001b[39m     \u001b[38;5;66;03m# In case one passes a config to `from_pretrained` + \"attn_implementation\"\u001b[39;00m\n\u001b[32m   3642\u001b[39m     \u001b[38;5;66;03m# override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3646\u001b[39m     \u001b[38;5;66;03m# we pop attn_implementation from the kwargs but this handles the case where users\u001b[39;00m\n\u001b[32m   3647\u001b[39m     \u001b[38;5;66;03m# passes manually the config to `from_pretrained`.\u001b[39;00m\n\u001b[32m   3648\u001b[39m     config = copy.deepcopy(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:568\u001b[39m, in \u001b[36mPretrainedConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[38;5;28mcls\u001b[39m.model_type:\n\u001b[32m    563\u001b[39m         logger.warning(\n\u001b[32m    564\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to instantiate a model of type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    565\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.model_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    566\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:734\u001b[39m, in \u001b[36mPretrainedConfig.from_dict\u001b[39m\u001b[34m(cls, config_dict, **kwargs)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[32m    732\u001b[39m config_dict[\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mpruned_heads\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    737\u001b[39m     config.pruned_heads = {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config.pruned_heads.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\.venv\\Lib\\site-packages\\transformers\\models\\llava_onevision\\configuration_llava_onevision.py:177\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, vision_config, text_config, image_token_index, video_token_index, projector_hidden_act, vision_feature_select_strategy, vision_feature_layer, vision_aspect_ratio, image_grid_pinpoints, tie_word_embeddings, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m vision_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    167\u001b[39m     vision_config = CONFIG_MAPPING[\u001b[33m\"\u001b[39m\u001b[33msiglip_vision_model\u001b[39m\u001b[33m\"\u001b[39m](\n\u001b[32m    168\u001b[39m         hidden_size=\u001b[32m1152\u001b[39m,\n\u001b[32m    169\u001b[39m         intermediate_size=\u001b[32m4304\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    174\u001b[39m         vision_use_head=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    175\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28mself\u001b[39m.vision_config = vision_config\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    180\u001b[39m     text_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] = text_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m text_config \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mqwen2\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:740\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extra_content[key]\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    741\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n\u001b[32m    742\u001b[39m module_name = model_type_to_module_name(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'qwen3'"
     ]
    }
   ],
   "source": [
    "# [필수] 메모리 파편화 방지 및 관리 설정\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "clean_memory()\n",
    "\n",
    "model_id = \"NCSOFT/VARCO-VISION-2.0-1.7B-OCR\"\n",
    "\n",
    "# 1. 4비트 양자화 + CPU 오프로딩 준비\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# 2. 모델 로드 (low_cpu_mem_usage 추가)\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    low_cpu_mem_usage=True # CPU 메모리 사용 최적화\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 3. 이미지 크기 극단적 축소 (성공 여부 확인용)\n",
    "image_path = \"test.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# 512나 768로 대폭 낮춥니다. (6GB에서는 1024도 벅찰 수 있습니다)\n",
    "target_size = 512 \n",
    "w, h = image.size\n",
    "if max(w, h) > target_size:\n",
    "    scale = target_size / max(w, h)\n",
    "    image = image.resize((int(w * scale), int(h * scale)), resample=Image.LANCZOS)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"<ocr>\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# 4. 입력 데이터 처리\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# 5. 추론 (더 쥐어짜기)\n",
    "clean_memory()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256, # 토큰 수를 확 줄여서 메모리 압박 해소\n",
    "        do_sample=False,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "# 6. 결과 출력\n",
    "output = processor.decode(generate_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "print(\"\\n=== OCR 결과 ===\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07205fb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ... (이전 추론 코드는 동일, max_new_tokens만 1024로 다시 늘려주세요)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     generate_ids = \u001b[43mmodel\u001b[49m.generate(\n\u001b[32m      7\u001b[39m         **inputs,\n\u001b[32m      8\u001b[39m         max_new_tokens=\u001b[32m1024\u001b[39m, \u001b[38;5;66;03m# 좌표값이 길어서 넉넉하게 줘야 끝까지 나옵니다.\u001b[39;00m\n\u001b[32m      9\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     10\u001b[39m         use_cache=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 결과 디코딩\u001b[39;00m\n\u001b[32m     14\u001b[39m full_output = processor.decode(generate_ids[\u001b[32m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(inputs.input_ids[\u001b[32m0\u001b[39m]):], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ... (이전 추론 코드는 동일, max_new_tokens만 1024로 다시 늘려주세요)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024, # 좌표값이 길어서 넉넉하게 줘야 끝까지 나옵니다.\n",
    "        do_sample=False,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "# 결과 디코딩\n",
    "full_output = processor.decode(generate_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "# 정규표현식을 사용해 숫자 좌표(0.123, 0.456 등)를 제거하는 함수\n",
    "def clean_ocr_text(text):\n",
    "    # 소수점 숫자가 포함된 좌표 패턴을 찾아 제거합니다.\n",
    "    cleaned = re.sub(r'\\d+\\.\\d+,\\s*\\d+\\.\\d+,\\s*\\d+\\.\\d+,\\s*\\d+\\.\\d+', '', text)\n",
    "    # 남은 숫자 조각들이나 불필요한 공백 정리\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "print(\"\\n=== 정제된 OCR 결과 ===\")\n",
    "print(clean_ocr_text(full_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
