from google import genai
import os
import re
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import TextLoader


# textfile loader ë¬´ê±°ìš´ ë²„ì „(ì „ì²˜ë¦¬ í•„ìš”í•  ê²½ìš°)
# from langchain_community.document_loaders import UnstructuredMarkdownLoader

# textfile loader ê°€ë²¼ìš´ ë²„ì „
from langchain_community.document_loaders import TextLoader

# Back í´ë”ì˜ .env íŒŒì¼ ë¡œë“œ
# ë¬¸ìì—´ë¡œ ê²½ë¡œ ì§ì ‘ ì§€ì • (rì„ ë¶™ì—¬ì•¼ ì—­ìŠ¬ë˜ì‹œ ì¸ì‹ì´ ì˜ ë©ë‹ˆë‹¤)
load_dotenv(r"..\Back\.env")
api_key = os.getenv("GOOGLE_API_KEY")
print(f"ë¡œë“œëœ API í‚¤ ì¡´ì¬ ì—¬ë¶€: {'ì˜ˆ' if api_key else 'ì•„ë‹ˆì˜¤'}")

# ì‚¬ì´íŠ¸ ì´ìš© ê°€ì´ë“œ md íŒŒì¼ load
current_dir = os.path.dirname(os.path.abspath(__file__))
md_file_path = os.path.join(current_dir, "guides", "member_website_guide.md")

loader = TextLoader(md_file_path, encoding='utf-8')
docs = loader.load()

# ì²˜ìŒ 100ìê¹Œì§€ë§Œ ì¶œë ¥í•´ë³´ê¸°
# print(docs[0].page_content[:100])


# ë§ˆí¬ë‹¤ìš´ split ê¸°ì¤€ (í˜ì´ì§€ ë‹¨ìœ„ì™€ ìƒì„¸ ì„¹ì…˜ ë‹¨ìœ„ë¥¼ ëª¨ë‘ í¬í•¨)
header_split_criterion = [
    ("##", "Page"),        # ëŒ€ì£¼ì œ: ë©”ì¸ í™ˆ, ë¡œê·¸ì¸, ë§ˆì´í˜ì´ì§€ ë“±
    ("###", "Section"),     # ì¤‘ì£¼ì œ: íˆì–´ë¡œ ì„¹ì…˜, íƒ­ êµ¬ì¡°, ì…ë ¥ í¼ ë“±
    ("####", "Subsection"),  # ì†Œì£¼ì œ: ìƒì„¸ ì…ë ¥ í•­ëª©, ë²„íŠ¼ ë™ì‘ ë“±
]


# ë§ˆí¬ë‹¤ìš´ì—ì„œ '##', '###'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1ì°¨ ë¶„í• 
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=header_split_criterion)
header_splits = markdown_splitter.split_text(docs[0].page_content)


# ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ 2ì°¨ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=100
)

splits = text_splitter.split_documents(header_splits)

# chunking ê²°ê³¼ í™•ì¸
# print(f"ì´ ì²­í¬ ê°œìˆ˜: {len(splits)}")
# print(f"ì²« ë²ˆì§¸ ì²­í¬ ë©”íƒ€ë°ì´í„°: {splits[1].metadata}")
# print(f"ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš©:\n{splits[1].page_content}")


# ì„ë² ë”© ëª¨ë¸
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ë²¡í„° db ìƒì„±
collection_name = "member_website_guide"
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    collection_name=collection_name,
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 5})


def extract_search_query(question: str) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— ë„ì›€ì´ ë˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë½‘ì•„ì„œ
    RAG ê²€ìƒ‰ì— ì‚¬ìš©í•  ì¿¼ë¦¬ ë¬¸ìì—´ì„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.
    """
    # í•œê¸€/ì˜ë¬¸/ìˆ«ì í† í°ë§Œ ì¶”ì¶œ
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", question)

    # ê¸°ì´ˆì ì¸ ë¶ˆìš©ì–´(ì¡°ì‚¬/ì ‘ì†ì–´ ë“±) ì œê±°
    stopwords = {
        "ëŠ”", "ì€", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ",
        "ë„", "ë§Œ", "ê¹Œì§€", "ë¶€í„°", "í•˜ê³ ", "ê·¼ë°", "ê·¸ë¦¬ê³ ", "ê·¸ëƒ¥",
        "í˜¹ì‹œ", "ì •ë§", "ì§„ì§œ", "ì¢€", "ì¡°ê¸ˆ", "ë„ˆë¬´", "ì–´ë–»ê²Œ", "ì™œ",
        "ê±°", "ìš”",
    }

    keywords = [t for t in tokens if t not in stopwords and len(t) > 1]

    # ë„ë©”ì¸ ê´€ë ¨ ë³´ì¡° í‚¤ì›Œë“œ(ì´ë¯¸ì§€/OCR ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œ)
    if any(k in question for k in ["OCR", "ocr", "ì´ë¯¸ì§€", "ì‚¬ì§„", "ì¸ì‹", "ê·¸ë¦¼ì¼ê¸°"]):
        keywords.extend(["OCR", "ì´ë¯¸ì§€", "ì‚¬ì§„", "ì¸ì‹", "ê·¸ë¦¼ì¼ê¸°"])

    # í‚¤ì›Œë“œê°€ í•˜ë‚˜ë„ ì•ˆ ë‚¨ìœ¼ë©´ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if not keywords:
        return question

    # í‚¤ì›Œë“œë“¤ì„ ê³µë°±ìœ¼ë¡œ ì´ì–´ì„œ ê²€ìƒ‰ìš© ì¿¼ë¦¬ë¡œ ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
    return " ".join(dict.fromkeys(keywords))


def retrieve_with_keywords(question: str):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ â†’ í‚¤ì›Œë“œ ì¶”ì¶œ â†’ í•´ë‹¹ í‚¤ì›Œë“œë¡œ RAG ê²€ìƒ‰ ì‹¤í–‰.
    """
    search_query = extract_search_query(question)
    return retriever.invoke(search_query)


llm = ChatGoogleGenerativeAI(
    model="models/gemini-flash-latest",
    temperature=0.2
)

template = """ë‹¹ì‹ ì€ 'ì•„ì´ë§ˆìŒ' ì›¹ì‚¬ì´íŠ¸ì˜ **ì „ë¬¸ ì´ìš© ê°€ì´ë“œ ì±—ë´‡**ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ `ë¬¸ì„œ íƒìƒ‰ ê²°ê³¼`ëŠ” `member_website_guied.md` íŒŒì¼ì„ RAGë¡œ ê²€ìƒ‰í•œ ê²°ê³¼ì´ë©°,
ê° ì„¹ì…˜ì€ ì›¹ì‚¬ì´íŠ¸ì˜ ì‹¤ì œ í™”ë©´ êµ¬ì¡°(í—¤ë”, í™ˆ, íšŒì›ê°€ì…/ë¡œê·¸ì¸, ê·¸ë¦¼ ë¶„ì„, ê·¸ë¦¼ì¼ê¸° OCR, ë§ˆì´í˜ì´ì§€, ì»¤ë®¤ë‹ˆí‹°, ìƒë‹´ì„¼í„° ì°¾ê¸°, FAQ ìš”ì•½ ë“±)ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.

[ì—­í• ]
- ë‹¹ì‹ ì€ **ì´ ë¬¸ì„œë¥¼ ê°€ì¥ ì˜ ì•„ëŠ” ì•ˆë‚´ ë‹´ë‹¹ì**ë¡œì„œ, ì‚¬ìš©ìê°€ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì–´ë–»ê²Œ ì´ìš©í•˜ë©´ ì¢‹ì„ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

[ì‘ë‹µ ê·œì¹™]
1. ë°˜ë“œì‹œ **ë¬¸ì„œ íƒìƒ‰ ê²°ê³¼(context)** ì•ˆì— ìˆëŠ” ì •ë³´ì™€ í‘œí˜„ì„ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì„¹ì…˜(###), í•˜ìœ„ ì„¹ì…˜(####)ì˜ ë‚´ìš©ì„ ê³¨ë¼, ê·¸ ë‚´ìš©ì„ **ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì¬êµ¬ì„±**í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
3. ì§ˆë¬¸ì´ ë¬¸ì„œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš°, **ì¶”ì¸¡í•´ì„œ ì§€ì–´ë‚´ì§€ ë§ê³ **, ë¬¸ì„œì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ê´€ë ¨ ë‚´ìš©ì„ ì•ˆë‚´í•´ ì£¼ì„¸ìš”.
4. ë²„íŠ¼/ìœ„ì¹˜/ê²½ë¡œì— ëŒ€í•´ì„œëŠ” **"ì–´ëŠ í™”ë©´ì—ì„œ, ì–´ë–¤ ë©”ë‰´/ë²„íŠ¼ì„ ëˆŒëŸ¬ì•¼ í•˜ëŠ”ì§€"** ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
5. ë‹µë³€ì€ ë°˜ë“œì‹œ **ê³µì†í•œ ë°˜ë§/ì¡´ì¤‘ì–´í†¤(ì˜ˆ: ~í•˜ì‹œë©´ ë©ë‹ˆë‹¤, ~í•´ ì£¼ì„¸ìš”)** ë¡œ ì‘ì„±í•˜ì„¸ìš”.
6. ë‹µë³€í•  ë•Œ ë¬¸ì„œ íƒìƒ‰ ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ëª¨ë¥´ê² ì„ ë•ŒëŠ” ì£¼ê´€ì ìœ¼ë¡œ ëŒ€ë‹µí•˜ì§€ ë§ê³  ë¬¸ì˜ ë©”ì¼(aimind@gmail.com)ì„ ì „ë‹¬í•˜ì„¸ìš”.

[ë¬¸ì„œ íƒìƒ‰ ê²°ê³¼]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}
"""

prompt = ChatPromptTemplate.from_template(template)


# ë²¡í„° db ìƒì„± ë˜ëŠ” ë¡œë“œ
collection_name="guied"
persist_dir = "./chroma_db"


if os.path.exists(persist_dir):
    # print('ì´ë¯¸ DB ì¡´ì¬í•¨')
    vectorstore = Chroma(
        persist_directory=persist_dir,
        embedding_function=embeddings,
        collection_name=collection_name
    )

else:
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=persist_dir
    )


retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# RAG Chain
rag_chain = (
    # RunnablePassthrough(): ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê°€ê³µ ì—†ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬
    # { "context": [ì°¾ì€ ë¬¸ì„œë“¤], "question": "ì‚¬ìš©ìì˜ ì§ˆë¬¸" }
    RunnableParallel({"context": retrieve_with_keywords, "question": RunnablePassthrough()})
    | prompt
    | llm
    # ë³µì¡í•œ llm ì‘ë‹µ ë°ì´í„°ì—ì„œ ì‚¬ìš©ìê°€ ì½ì„ ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ, ì¶œë ¥í•´ì£¼ëŠ” parser
    | StrOutputParser() 
)

question = input("ì›¹ ì‚¬ì´íŠ¸ ì´ìš© ë°©ë²•ì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”!  ")

# 2. í‚¤ì›Œë“œ ê¸°ë°˜ retrieverë¥¼ í†µí•´ ê´€ë ¨ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
search_results = retrieve_with_keywords(question)

# 3. retriever ê²°ê³¼ í™•ì¸
print(f"\nğŸ” '{question}'ì— ëŒ€í•´ ì°¾ì€ ë¬¸ì„œ ê°œìˆ˜: {len(search_results)}ê°œ\n")

for i, doc in enumerate(search_results):
    print(f"--- [ê²€ìƒ‰ ê²°ê³¼ {i+1}] ---")
    print(f"ë‚´ìš© ìš”ì•½: {doc.page_content[:200]}...")  # ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")
    print("\n")

answer = rag_chain.invoke(question)
print('ë‹µë³€:', answer)

# ì§ˆë¬¸: ê·¸ë¦¼ ì¸ì‹ì„ ë” ì˜ ì‹œí‚¤ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?



# ì‹¬ë¦¬ ë¶„ì„ ê²°ê³¼ ì±—ë´‡
