{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d64572a",
      "metadata": {},
      "source": [
        "# ê·¸ë¦¼ì¼ê¸° í…ìŠ¤íŠ¸ íƒì§€Â·ì¸ì‹ íŒŒì´í”„ë¼ì¸ (VARCO-VISION-2.0-1.7B-OCR)\n",
        "\n",
        "ì•„ë™ ì‹¬ë¦¬ ë¶„ì„ìš© ê·¸ë¦¼ì¼ê¸° ì´ë¯¸ì§€ì—ì„œ **í…ìŠ¤íŠ¸ ì˜ì—­ì„ íƒì§€**í•˜ê³  **OCR**ë¡œ ì¸ì‹í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "- **ì „ì²˜ë¦¬**: ì œê³µëœ ë¬¸ì„œ ë³´ì •Â·ì •ë¦¬ ì½”ë“œ í¬í•¨ (4ì  ë³€í™˜, adaptive cleanup)\n",
        "- **ëª¨ë¸**: NCSOFT VARCO-VISION-2.0-1.7B-OCR\n",
        "- **VRAM**: 6GB ëŒ€ì‘ì„ ìœ„í•œ 4-bit ì–‘ìí™”(bitsandbytes) ì ìš©"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c0ce7b",
      "metadata": {},
      "source": [
        "## 1. í™˜ê²½ í™•ì¸ (CUDA / VRAM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85f2910",
      "metadata": {},
      "source": [
        "**GPU ì‚¬ìš© ê°€ëŠ¥: Falseì¸ ê²½ìš°**  \n",
        "1. **ì»¤ë„ í™•ì¸**: ìš°ì¸¡ ìƒë‹¨ ì»¤ë„ì´ í”„ë¡œì íŠ¸ **venv**ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. (ë‹¤ë¥¸ Pythonì´ë©´ GPUìš© torchê°€ ì—†ì„ ìˆ˜ ìˆìŒ)  \n",
        "2. **í„°ë¯¸ë„**ì—ì„œ (Jupyter/Cursor ëª¨ë‘ ëˆ ë’¤):\n",
        "   ```bash\n",
        "   cd c:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\n",
        "   .\\venv\\Scripts\\activate\n",
        "   pip uninstall torch torchvision -y\n",
        "   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "   ```\n",
        "3. Cursor/Jupyter ë‹¤ì‹œ ì¼œê³ , **ì»¤ë„ì„ ì´ í”„ë¡œì íŠ¸ venvë¡œ ì„ íƒ**í•œ ë’¤ **ì»¤ë„ ì¬ì‹œì‘** â†’ \"í™˜ê²½ í™•ì¸\" ì…€ ë‹¤ì‹œ ì‹¤í–‰."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "31355e5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: c:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Scripts\\python.exe\n",
            "torch ìœ„ì¹˜: c:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\torch\\__init__.py\n",
            "torch CUDA ë²„ì „: 11.8\n",
            "\n",
            "GPU ì‚¬ìš© ê°€ëŠ¥: True\n",
            "ì¥ì¹˜: NVIDIA GeForce RTX 3060 Laptop GPU\n",
            "VRAM: 6.44 GB\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "\n",
        "# í˜„ì¬ ì»¤ë„ì´ ì“°ëŠ” Python (venvì¸ì§€ í™•ì¸)\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"torch ìœ„ì¹˜: {torch.__file__}\")\n",
        "print(f\"torch CUDA ë²„ì „: {torch.version.cuda}\")  # Noneì´ë©´ CPU ì „ìš© ë¹Œë“œ\n",
        "print()\n",
        "\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {cuda_available}\")\n",
        "\n",
        "if cuda_available:\n",
        "    current_device = torch.cuda.current_device()\n",
        "    device_name = torch.cuda.get_device_name(current_device)\n",
        "    total_mem = torch.cuda.get_device_properties(current_device).total_memory / 1e9\n",
        "    print(f\"ì¥ì¹˜: {device_name}\")\n",
        "    print(f\"VRAM: {total_mem:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ ì‹œ ë©”ëª¨ë¦¬Â·ì†ë„ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    if torch.version.cuda is None:\n",
        "        print(\"   â†’ torch.version.cudaê°€ None: CPU ì „ìš© PyTorchê°€ ì„¤ì¹˜ëœ ìƒíƒœì…ë‹ˆë‹¤. ì•„ë˜ 'GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ' ì•ˆë‚´ë¥¼ ë”°ë¥´ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf294db",
      "metadata": {},
      "source": [
        "## 2. ì˜ì¡´ì„± ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ)\n",
        "\n",
        "**CUDA 11.8 + í•œêµ­ì–´ OCR(VARCO) í˜¸í™˜ ë²„ì „.** ì•„ë˜ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "de41a71c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [1] PyTorch (CUDA 11.8 ì „ìš©) - ë¨¼ì € ì‹¤í–‰\n",
        "# %pip uninstall torch torchvision -y\n",
        "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# [2] ë‚˜ë¨¸ì§€ íŒ¨í‚¤ì§€ (transformers 4.53.xë§Œ - 4.54+ëŠ” torch._export í˜¸í™˜ ë¬¸ì œ)\n",
        "# %pip install \"transformers>=4.53.1,<4.54\" \"opencv-python>=4.8.0\" \"matplotlib>=3.7\" \"Pillow>=10.0\" \"numpy>=1.24\" \"bitsandbytes>=0.43.0\" \"accelerate>=0.30.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "08c5cbd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# \"No module named 'matplotlib'\" ë“± ë‚˜ì˜¤ë©´ ì´ ì…€ë§Œ ì‹¤í–‰í•œ ë’¤, ì•„ë˜ \"ì„í¬íŠ¸\" ì…€ ë‹¤ì‹œ ì‹¤í–‰\n",
        "# %pip install matplotlib opencv-python numpy Pillow --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36333536",
      "metadata": {},
      "source": [
        "## 2. ì„í¬íŠ¸ ë° ëª¨ë¸ ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "58c1a3f2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ëª¨ë¸ ë¡œë”© ì¤‘ (4-bit)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\n"
          ]
        }
      ],
      "source": [
        "# CUDA 11.8 + í•œêµ­ì–´ OCR í˜¸í™˜\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "def clean_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "clean_memory()\n",
        "\n",
        "# ---------- ëª¨ë¸ ë¡œë“œ ----------\n",
        "if \"model\" in globals():\n",
        "    del model\n",
        "clean_memory()\n",
        "model_id = \"NCSOFT/VARCO-VISION-2.0-1.7B-OCR\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "print(\"ëª¨ë¸ ë¡œë”© ì¤‘ (4-bit)...\")\n",
        "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"sdpa\",\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "torch.backends.cudnn.benchmark = True  # ì…ë ¥ í¬ê¸°ê°€ ë¹„ìŠ·í•  ë•Œ ì¶”ë¡  ì†ë„ í–¥ìƒ\n",
        "print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\")\n",
        "\n",
        "# ---------- OCR í—¬í¼ í•¨ìˆ˜ ----------\n",
        "def parse_ocr_output(output_text):\n",
        "    text_boxes = []\n",
        "    full_text_parts = []\n",
        "    pattern = re.compile(r\"<char>\\s*([^<]*?)\\s*</char>\\s*<bbox>\\s*([^<]*?)\\s*</bbox>\", re.DOTALL)\n",
        "    for m in pattern.finditer(output_text):\n",
        "        text = m.group(1).strip()\n",
        "        bbox_str = m.group(2).strip()\n",
        "        full_text_parts.append(text)\n",
        "        try:\n",
        "            coords = [float(x.strip()) for x in bbox_str.split(\",\")]\n",
        "            if len(coords) >= 4:\n",
        "                text_boxes.append({\"text\": text, \"bbox\": coords[:4]})\n",
        "        except (ValueError, AttributeError):\n",
        "            text_boxes.append({\"text\": text, \"bbox\": None})\n",
        "    full_text = \"\".join(full_text_parts) if full_text_parts else output_text\n",
        "    return text_boxes, full_text\n",
        "\n",
        "def get_diary_text(raw_output, full_text):\n",
        "    s = (full_text or \"\").strip()\n",
        "    if len(s) > 4:\n",
        "        from collections import Counter\n",
        "        c = Counter(s.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
        "        most = c.most_common(1)[0] if c else (\"\", 0)\n",
        "        if most[1] >= len(s) * 0.5:\n",
        "            raw_clean = re.sub(r\"<char>.*?</char>\", \"\", raw_output or \"\", flags=re.DOTALL)\n",
        "            raw_clean = re.sub(r\"<bbox>.*?</bbox>\", \"\", raw_clean, flags=re.DOTALL)\n",
        "            raw_clean = raw_clean.replace(\"<ocr>\", \"\").strip()\n",
        "            return raw_clean if raw_clean else (raw_output or full_text)\n",
        "    return full_text or raw_output or \"\"\n",
        "\n",
        "# pil_image, max_long_side => ì´ë¯¸ì§€ í¬ê¸°,  max_new_tokens => ê¸€ì”¨ ì¸ì‹ ê¸¸ì´\n",
        "def run_varco_ocr(pil_image, max_long_side=800, max_new_tokens=1400, diary_mode=False):\n",
        "    # diary_mode=False: ê³µì‹ í”„ë¡¬í”„íŠ¸ \"<ocr>\"ë§Œ ì‚¬ìš© â†’ ëª¨ë¸ì´ <char>...</char><bbox>...</bbox> í˜•ì‹ìœ¼ë¡œ ì¶œë ¥(ë°•ìŠ¤ íƒì§€ ê°€ëŠ¥)\n",
        "    # diary_mode=True: ê·¸ë¦¼ì¼ê¸° ì•ˆë‚´ ë¬¸ì¥ ì‚¬ìš© â†’ ë¬¸ì¥ë§Œ ë‚˜ì˜¬ ìˆ˜ ìˆì–´ ë°•ìŠ¤ê°€ 0ê°œì¼ ìˆ˜ ìˆìŒ\n",
        "    w, h = pil_image.size\n",
        "    if max(w, h) > max_long_side:\n",
        "        scale = max_long_side / max(w, h)\n",
        "        new_w, new_h = int(w * scale), int(h * scale)\n",
        "        pil_image = pil_image.resize((new_w, new_h), resample=Image.LANCZOS)\n",
        "    pil_image_sent = pil_image\n",
        "    ocr_prompt = \"<ocr>\\nì´ë¯¸ì§€ì— ì†ìœ¼ë¡œ ì“´ ê·¸ë¦¼ì¼ê¸° ê¸€ì„ ìœ„ì—ì„œ ì•„ë˜ë¡œ í•œ ì¤„ì”© ê·¸ëŒ€ë¡œ ì½ì–´ì„œ ì¶œë ¥í•´ì¤˜.\" if diary_mode else \"<ocr>\"\n",
        "    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": pil_image}, {\"type\": \"text\", \"text\": ocr_prompt}]}]\n",
        "    inputs = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():\n",
        "        generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, num_beams=1, use_cache=True)\n",
        "    generated = generate_ids[0][len(inputs.input_ids[0]):]\n",
        "    raw_output = processor.decode(generated, skip_special_tokens=False)\n",
        "    text_boxes, full_text = parse_ocr_output(raw_output)\n",
        "    return raw_output, text_boxes, full_text, pil_image_sent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4679017e",
      "metadata": {},
      "source": [
        "## 3. ì „ì²˜ë¦¬\n",
        "\n",
        "ë¬¸ì„œ ì˜ì—­ íƒì§€Â·ë³´ì •(4ì  ë³€í™˜) + adaptive cleanup + preprocess_diary_image (í•œ ì…€ì— ëª¨ë‘ í¬í•¨)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1902bc57",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ.\n"
          ]
        }
      ],
      "source": [
        "def imshow(title, image):\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.title(title)\n",
        "    if len(image.shape) == 3:\n",
        "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def order_points(pts):\n",
        "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
        "    s = pts.sum(axis=1)\n",
        "    rect[0] = pts[np.argmin(s)]\n",
        "    rect[2] = pts[np.argmax(s)]\n",
        "    diff = np.diff(pts, axis=1)\n",
        "    rect[1] = pts[np.argmin(diff)]\n",
        "    rect[3] = pts[np.argmax(diff)]\n",
        "    return rect\n",
        "\n",
        "def four_point_transform(image, pts):\n",
        "    rect = order_points(pts)\n",
        "    (tl, tr, br, bl) = rect\n",
        "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "    maxWidth = max(int(widthA), int(widthB))\n",
        "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "    maxHeight = max(int(heightA), int(heightB))\n",
        "    dst = np.array([[0, 0], [maxWidth - 1, 0], [maxWidth - 1, maxHeight - 1], [0, maxHeight - 1]], dtype=\"float32\")\n",
        "    M = cv2.getPerspectiveTransform(rect, dst)\n",
        "    return cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
        "\n",
        "def expand_contour(cnt, scale=1.03):\n",
        "    M = cv2.moments(cnt)\n",
        "    if M['m00'] == 0:\n",
        "        return cnt\n",
        "    cx = int(M['m10'] / M['m00'])\n",
        "    cy = int(M['m01'] / M['m00'])\n",
        "    cnt_norm = cnt - [cx, cy]\n",
        "    cnt_scaled = cnt_norm * scale\n",
        "    cnt_new = cnt_scaled + [cx, cy]\n",
        "    return cnt_new.astype(np.int32)\n",
        "\n",
        "def adaptive_cleanup(image):\n",
        "    if len(image.shape) == 3:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        gray = image.copy()\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 40))\n",
        "    bg_estimation = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
        "    normalized = cv2.divide(gray, bg_estimation, scale=255)\n",
        "    # ê¸€ì”¨ ì§„í•˜ê¸°ê¸°\n",
        "    smoothed = cv2.bilateralFilter(normalized, d=3, sigmaColor=30, sigmaSpace=30)\n",
        "    otsu_thresh, _ = cv2.threshold(smoothed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    # í°ìƒ‰ì˜ì—­ì—­\n",
        "    cutoff = min(otsu_thresh + 35, 255)\n",
        "    _, clean_result = cv2.threshold(smoothed, cutoff, 255, cv2.THRESH_TRUNC)\n",
        "    final_output = cv2.normalize(clean_result, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    # (6) ì¤„ ì œê±°: ì—°ê²° ìš”ì†Œ ê¸¸ì´Â·ë¹„ìœ¨ë¡œ ê°€ë¡œ/ì„¸ë¡œ ì„ ë§Œ ì œê±° (ê¸€ìë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ê¸´ ê²ƒë§Œ)\n",
        "\n",
        "    _, binary_inv = cv2.threshold(final_output, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_inv, connectivity=8)\n",
        "    h_img, w_img = binary_inv.shape[0], binary_inv.shape[1]\n",
        "    min_len_h = max(50, int(0.15 * w_img))\n",
        "    min_len_v = max(50, int(0.15 * h_img))\n",
        "    aspect = 4\n",
        "    no_lines = binary_inv.copy()\n",
        "    for i in range(1, num_labels):\n",
        "        w, h = int(stats[i, 2]), int(stats[i, 3])\n",
        "        if w >= aspect * h and w >= min_len_h:\n",
        "            no_lines[labels == i] = 0\n",
        "        elif h >= aspect * w and h >= min_len_v:\n",
        "            no_lines[labels == i] = 0\n",
        "    # (7) ê¸€ì êµµê²Œ: dilation (ì¤„ ì œê±° í›„ ì ìš©)\n",
        "    kernel_thick = np.ones((1, 1), np.uint8)\n",
        "    thickened = cv2.dilate(no_lines, kernel_thick)\n",
        "    final_output = 255 - thickened\n",
        "    return final_output\n",
        "\n",
        "def preprocess_diary_image(file_path, show_result=False, use_color_for_ocr=True, return_detection_vis=False):\n",
        "    \"\"\"ë¬¸ì„œ ì˜ì—­ ë³´ì • + cleanup. return_detection_vis=Trueë©´ ì›ë³¸ì— íƒì§€ëœ ë¬¸ì„œ ì˜ì—­(4ê°í˜•) ê·¸ë¦° ì´ë¯¸ì§€ë„ ë°˜í™˜.\"\"\"\n",
        "    image = cv2.imread(file_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
        "    orig = image.copy()\n",
        "    ratio = image.shape[0] / 500.0\n",
        "    h, w = 500, int(image.shape[1] / ratio)\n",
        "    image_resized = cv2.resize(image, (w, h))\n",
        "    gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    edged = cv2.Canny(gray, 50, 150)\n",
        "    kernel_edge = np.ones((5, 5), np.uint8)\n",
        "    edged = cv2.dilate(edged, kernel_edge, iterations=1)\n",
        "    cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:5]\n",
        "    screenCnt = None\n",
        "    for c in cnts:\n",
        "        peri = cv2.arcLength(c, True)\n",
        "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
        "        if len(approx) == 4:\n",
        "            screenCnt = approx\n",
        "            break\n",
        "    # 4ê°í˜• ëª» ì°¾ìœ¼ë©´ ì›ë³¸ ì´ë¯¸ì§€ ê·¸ëŒ€ë¡œ ì‚¬ìš© (minAreaRect fallback ì œê±°)\n",
        "    detection_vis_bgr = None\n",
        "    if screenCnt is None:\n",
        "        warped = orig\n",
        "        if return_detection_vis:\n",
        "            detection_vis_bgr = orig.copy()\n",
        "            cv2.putText(detection_vis_bgr, \"Document not detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)\n",
        "    else:\n",
        "        screenCnt = expand_contour(screenCnt, scale=1.03)\n",
        "        pts_orig = (screenCnt.reshape(4, 2) * ratio).astype(np.int32)\n",
        "        warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)\n",
        "        if return_detection_vis:\n",
        "            detection_vis_bgr = orig.copy()\n",
        "            cv2.polylines(detection_vis_bgr, [pts_orig], True, (0, 255, 0), 3)\n",
        "            for i, pt in enumerate(pts_orig):\n",
        "                cv2.circle(detection_vis_bgr, tuple(pt), 8, (0, 0, 255), -1)\n",
        "                cv2.putText(detection_vis_bgr, str(i+1), tuple(pt), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
        "    # ê°€ë¡œ/ì„¸ë¡œ íŒë³„: ìŠ¤ìº”ëœ ì‚¬ê°í˜•ì˜ ê°€ë¡œÂ·ì„¸ë¡œ ë¹„êµ\n",
        "    h_warped, w_warped = warped.shape[:2]\n",
        "    is_horizontal = w_warped > h_warped\n",
        "    if is_horizontal:\n",
        "        # === Horizontal version: ì™¼ìª½(ìƒë‹¨20%Â·ì¢Œì¸¡55%) â†’ ì˜¤ë¥¸ìª½ ìˆœì„œë¡œ OCR ===\n",
        "        left_half = warped[:, :w_warped//2]\n",
        "        right_half = warped[:, w_warped//2:]\n",
        "        h_left, w_left = left_half.shape[:2]\n",
        "        # ì™¼ìª½ ë¶€ë¶„ ì¤‘ ìƒë‹¨ 20%, ì¢Œì¸¡ 55%ë§Œ ì‚¬ìš©\n",
        "        left_cropped = left_half[0:int(h_left * 0.2), 0:int(w_left * 0.55)]\n",
        "        left_clean = adaptive_cleanup(left_cropped)\n",
        "        right_clean = adaptive_cleanup(right_half)\n",
        "        left_pil = Image.fromarray(cv2.cvtColor(left_clean, cv2.COLOR_GRAY2RGB))\n",
        "        right_pil = Image.fromarray(cv2.cvtColor(right_clean, cv2.COLOR_GRAY2RGB))\n",
        "        h_left, w_left = left_clean.shape[:2]\n",
        "        h_right, w_right = right_clean.shape[:2]\n",
        "        if h_left != h_right:\n",
        "            pad_height = h_right - h_left\n",
        "            left_padded = np.vstack([left_clean, np.ones((pad_height, w_left), dtype=left_clean.dtype) * 255])\n",
        "            final_bgr = cv2.cvtColor(np.hstack([left_padded, right_clean]), cv2.COLOR_GRAY2BGR)\n",
        "        else:\n",
        "            final_bgr = cv2.cvtColor(np.hstack([left_clean, right_clean]), cv2.COLOR_GRAY2BGR)\n",
        "        if return_detection_vis and detection_vis_bgr is not None:\n",
        "            cv2.putText(detection_vis_bgr, \"Horizontal: left(top20%,left55%) -> right OCR\", (50, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 165, 255), 2)\n",
        "        if show_result:\n",
        "            imshow(\"Left (top20%, left55%)\", left_clean)\n",
        "            imshow(\"Right page\", right_clean)\n",
        "        det = detection_vis_bgr if return_detection_vis else None\n",
        "        return left_pil, right_pil, final_bgr, det, \"horizontal\"\n",
        "    else:\n",
        "        # === Vertical version: ì„¸ë¡œë¡œ ê¸¸ë©´ ì „ì²´ ì‚¬ìš© ===\n",
        "        if return_detection_vis and detection_vis_bgr is not None:\n",
        "            cv2.putText(detection_vis_bgr, \"Vertical version: full image\", (50, 90), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 165, 255), 2)\n",
        "        final_result = adaptive_cleanup(warped)\n",
        "        final_bgr = cv2.cvtColor(final_result, cv2.COLOR_GRAY2BGR)\n",
        "        pil_image = Image.fromarray(cv2.cvtColor(final_result, cv2.COLOR_GRAY2RGB))\n",
        "        if show_result:\n",
        "            imshow(\"Smart Clean Scan Result\", final_result)\n",
        "        if return_detection_vis and detection_vis_bgr is not None:\n",
        "            return pil_image, final_bgr, detection_vis_bgr, \"vertical\"\n",
        "        return pil_image, final_bgr, \"vertical\"\n",
        "\n",
        "print(\"ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7165ad1",
      "metadata": {},
      "source": [
        "## 4. ì‹¤í–‰\n",
        "\n",
        "ì•„ë˜ ì…€ë§Œ ì‹¤í–‰í•˜ë©´ ë¨: `file_path` ì„¤ì • í›„ ì „ì²˜ë¦¬ â†’ OCR â†’ ê²°ê³¼Â·ë°”ìš´ë”© ë°•ìŠ¤ í‘œì‹œ/ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b749aa48",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ ì˜¤ë¥˜: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 550 and the array at index 1 has size 2752\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\jwhon\\AppData\\Local\\Temp\\ipykernel_26804\\2436590483.py\", line 18, in <module>\n",
            "    result = preprocess_diary_image(file_path, show_result=show_preprocess, return_detection_vis=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\jwhon\\AppData\\Local\\Temp\\ipykernel_26804\\711018591.py\", line 136, in preprocess_diary_image\n",
            "    final_bgr = cv2.cvtColor(np.hstack([left_clean, right_clean]), cv2.COLOR_GRAY2BGR)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\ddddwoo\\project\\Oneness\\AiMind-AiModels\\venv\\Lib\\site-packages\\numpy\\_core\\shape_base.py\", line 367, in hstack\n",
            "    return _nx.concatenate(arrs, 1, dtype=dtype, casting=casting)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 550 and the array at index 1 has size 2752\n"
          ]
        }
      ],
      "source": [
        "# ========== ì„¤ì •: ê·¸ë¦¼ì¼ê¸° ì´ë¯¸ì§€ ê²½ë¡œ ==========\n",
        "file_path = \"2page3.jpg\"\n",
        "max_long_side = 512   # ê·¸ë¦¼ì¼ê¸° í•œ ì¤„ì”© ì½ê¸°: 2048 ê¶Œì¥. VRAM ë¶€ì¡±í•˜ë©´ 1536ìœ¼ë¡œ ë‚®ì¶”ê¸°\n",
        "show_preprocess = True  # Trueë©´ ì „ì²˜ë¦¬ ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ\n",
        "show_detection = True   # Trueë©´ ë¬¸ì„œ ì˜ì—­ íƒì§€ ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ\n",
        "save_detection_images = False  # Trueë©´ íƒì§€ ì´ë¯¸ì§€ ì €ì¥ (bboxëŠ” ì €ì¥ ì•ˆ í•¨, í‘œì‹œë§Œ)\n",
        "diary_mode = False  # Falseë©´ <char><bbox> í˜•ì‹ìœ¼ë¡œ ë°•ìŠ¤ íƒì§€. Trueë©´ ê·¸ë¦¼ì¼ê¸° ë¬¸ì¥ ìœ„ì£¼(ë°•ìŠ¤ ì—†ì„ ìˆ˜ ìˆìŒ)\n",
        "use_character_regions = False  # True: ê¸€ì ì˜ì—­ ë¨¼ì € ì°¾ê³  ì˜ì—­ë³„ OCR í›„ ì—´ ìˆœ ì •ë ¬ (2ë²ˆ ë°©ì‹)\n",
        "# ===============================================\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
        "else:\n",
        "    try:\n",
        "        import warnings\n",
        "        warnings.filterwarnings(\"ignore\", message=\".*[Gg]lyph.*missing from font\", category=UserWarning)\n",
        "        # 1) ì „ì²˜ë¦¬ (ë¬¸ì„œ ì˜ì—­ íƒì§€ + ë³´ì •)\n",
        "        result = preprocess_diary_image(file_path, show_result=show_preprocess, return_detection_vis=True)\n",
        "        if len(result) == 5:\n",
        "            # Horizontal: ì™¼ìª½ â†’ ì˜¤ë¥¸ìª½ ìˆœì„œë¡œ OCR\n",
        "            left_pil, right_pil, preprocessed_bgr, detection_vis_bgr, version = result\n",
        "            raw_left, text_boxes_left, full_text_left, pil_left_sent = run_varco_ocr(left_pil, max_long_side=max_long_side, diary_mode=diary_mode)\n",
        "            raw_right, text_boxes_right, full_text_right, pil_right_sent = run_varco_ocr(right_pil, max_long_side=max_long_side, diary_mode=diary_mode)\n",
        "            full_text = full_text_left + full_text_right\n",
        "            raw_output = (raw_left or \"\") + \"\\n\" + (raw_right or \"\")\n",
        "            text_boxes = text_boxes_left.copy()\n",
        "            w_left = pil_left_sent.size[0]\n",
        "            w_right = pil_right_sent.size[0]\n",
        "            w_comb = w_left + w_right\n",
        "            h_comb = max(pil_left_sent.size[1], pil_right_sent.size[1])\n",
        "            for item in text_boxes_right:\n",
        "                b = item.get(\"bbox\")\n",
        "                if b and len(b) >= 4 and max(b) <= 1.0:\n",
        "                    x1, y1, x2, y2 = b[0], b[1], b[2], b[3]\n",
        "                    x1_new = (x1 * w_right + w_left) / w_comb\n",
        "                    x2_new = (x2 * w_right + w_left) / w_comb\n",
        "                    text_boxes.append({\"text\": item[\"text\"], \"bbox\": [x1_new, y1, x2_new, y2]})\n",
        "                else:\n",
        "                    text_boxes.append(item)\n",
        "            pil_image_sent = Image.new(\"RGB\", (w_comb, h_comb), (255, 255, 255))\n",
        "            pil_image_sent.paste(pil_left_sent, (0, 0))\n",
        "            pil_image_sent.paste(pil_right_sent, (w_left, 0))\n",
        "        elif len(result) == 4:\n",
        "            pil_image, preprocessed_bgr, detection_vis_bgr, version = result\n",
        "            raw_output, text_boxes, full_text, pil_image_sent = run_varco_ocr(pil_image, max_long_side=max_long_side, diary_mode=diary_mode)\n",
        "        elif len(result) == 3:\n",
        "            pil_image, preprocessed_bgr, version = result\n",
        "            detection_vis_bgr = None\n",
        "            raw_output, text_boxes, full_text, pil_image_sent = run_varco_ocr(pil_image, max_long_side=max_long_side, diary_mode=diary_mode)\n",
        "        else:\n",
        "            pil_image, preprocessed_bgr = result\n",
        "            detection_vis_bgr = None\n",
        "            version = None\n",
        "            raw_output, text_boxes, full_text, pil_image_sent = run_varco_ocr(pil_image, max_long_side=max_long_side, diary_mode=diary_mode)\n",
        "        print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {file_path}\" + (f\" ({version} version)\" if version else \"\"))\n",
        "\n",
        "        # 1-1) ë¬¸ì„œ ì˜ì—­ íƒì§€ ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ/ì €ì¥\n",
        "        if detection_vis_bgr is not None:\n",
        "            if show_detection:\n",
        "                plt.figure(figsize=(12, 12))\n",
        "                plt.title(\"ğŸ“ ë¬¸ì„œ ì˜ì—­ íƒì§€ ê²°ê³¼ (ì´ˆë¡=íƒì§€ëœ 4ê°í˜•, ë¹¨ê°•=ê¼­ì§“ì )\")\n",
        "                plt.imshow(cv2.cvtColor(detection_vis_bgr, cv2.COLOR_BGR2RGB))\n",
        "                plt.axis(\"off\")\n",
        "                plt.show()\n",
        "            if save_detection_images:\n",
        "                out_path = file_path.rsplit(\".\", 1)[0] + \"_detection.jpg\"\n",
        "                cv2.imwrite(out_path, detection_vis_bgr)\n",
        "                print(f\"ğŸ’¾ íƒì§€ ê²°ê³¼ ì €ì¥: {out_path}\")\n",
        "\n",
        "        # 2) VARCO OCR (horizontalì€ ìœ„ì—ì„œ ì´ë¯¸ ì‹¤í–‰ë¨)\n",
        "        if not text_boxes and (raw_output or \"\").strip():\n",
        "            print(\"âš ï¸ í…ìŠ¤íŠ¸ ë°•ìŠ¤ 0ê°œ. ì›ì¸ í™•ì¸ìš© ëª¨ë¸ ì›ì‹œ ì¶œë ¥ ì¼ë¶€:\\n\", (raw_output or \"\")[:1000])\n",
        "\n",
        "        # 2-1) ì‘ì€ ê¸€ì”¨ ì œì™¸: bbox ë©´ì ì´ min_bbox_area ë¯¸ë§Œì´ë©´ ì œê±°\n",
        "        min_bbox_area = 0.01\n",
        "        text_boxes = [item for item in text_boxes\n",
        "                      if item.get(\"bbox\") and len(item[\"bbox\"]) >= 4\n",
        "                      and (item[\"bbox\"][2]-item[\"bbox\"][0]) * (item[\"bbox\"][3]-item[\"bbox\"][1]) >= min_bbox_area]\n",
        "        full_text = \"\".join(item[\"text\"] for item in text_boxes)\n",
        "\n",
        "        # 3) ê²°ê³¼ ì¶œë ¥\n",
        "        display_text = get_diary_text(raw_output, full_text)\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ê·¸ë¦¼ì¼ê¸° í•œ ì¤„ì”©)\")\n",
        "        print(\"=\" * 50)\n",
        "        print(display_text if display_text.strip() else raw_output)\n",
        "        print(\"=\" * 50)\n",
        "        if text_boxes:\n",
        "            print(\"\\nğŸ“¦ í…ìŠ¤íŠ¸ ë°•ìŠ¤ (ë¬¸ì/ë‹¨ì–´ë³„ ì¢Œí‘œ)\")\n",
        "            for i, item in enumerate(text_boxes[:20]):  # ìƒìœ„ 20ê°œë§Œ ì¶œë ¥\n",
        "                print(f\"  [{i+1}] {item['text']!r} -> bbox: {item['bbox']}\")\n",
        "            if len(text_boxes) > 20:\n",
        "                print(f\"  ... ì™¸ {len(text_boxes) - 20}ê°œ\")\n",
        "\n",
        "        # 4) ê¸€ì ìœ„ì¹˜ íƒì§€ ê²°ê³¼: ëª¨ë¸ì´ ë³¸ ì´ë¯¸ì§€ ìœ„ì— ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì €ì¥ ì•ˆ í•¨, í‘œì‹œë§Œ)\n",
        "        vis = cv2.cvtColor(np.array(pil_image_sent), cv2.COLOR_RGB2BGR)\n",
        "        h_vis, w_vis = vis.shape[:2]\n",
        "        if text_boxes:\n",
        "            for item in text_boxes:\n",
        "                bbox = item.get(\"bbox\")\n",
        "                if bbox is None or len(bbox) < 4:\n",
        "                    continue\n",
        "                x1, y1, x2, y2 = bbox\n",
        "                if max(x1, y1, x2, y2) <= 1.0:\n",
        "                    x1, x2 = int(x1 * w_vis), int(x2 * w_vis)\n",
        "                    y1, y2 = int(y1 * h_vis), int(y2 * h_vis)\n",
        "                else:\n",
        "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "        print(\"\\nğŸ“ ê¸€ì ìœ„ì¹˜ íƒì§€ ê²°ê³¼ (ë°”ìš´ë”© ë°•ìŠ¤) - ëª¨ë¸ì´ ë³¸ ì´ë¯¸ì§€ ìœ„ì— íƒì§€ëœ ê¸€ì ì˜ì—­ì„ ê·¸ë ¸ìŠµë‹ˆë‹¤.\")\n",
        "        plt.figure(figsize=(12, 12))\n",
        "        plt.title(\"OCR Bounding Boxes\")\n",
        "        plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        # 5) easy.py ìŠ¤íƒ€ì¼ íƒì§€ í™•ì¸: PILë¡œ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ê·¸ë¦¬ê¸° (polygon, ìƒ‰ìœ¼ë¡œ êµ¬ë¶„)\n",
        "        from PIL import ImageDraw\n",
        "        img_draw = pil_image_sent.copy().convert(\"RGB\")\n",
        "        draw = ImageDraw.Draw(img_draw)\n",
        "        w_img, h_img = img_draw.size\n",
        "        for i, item in enumerate(text_boxes or []):\n",
        "            bbox = item.get(\"bbox\")\n",
        "            if bbox is None or len(bbox) < 4:\n",
        "                continue\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            if max(x1, y1, x2, y2) <= 1.0:\n",
        "                x1, x2 = x1 * w_img, x2 * w_img\n",
        "                y1, y2 = y1 * h_img, y2 * h_img\n",
        "            box = [(int(x1), int(y1)), (int(x2), int(y1)), (int(x2), int(y2)), (int(x1), int(y2))]\n",
        "            # easy.pyì²˜ëŸ¼: ë¹¨ê°•(255,0,0) / ì´ˆë¡(0,255,0) êµ¬ë¶„ (VARCOëŠ” prob ì—†ìŒ â†’ ê¸€ì ìˆ˜ë¡œ ëŒ€ì‹ )\n",
        "            color = (255, 0, 0) if len(item.get(\"text\", \"\").strip()) > 1 else (0, 255, 0)\n",
        "            draw.polygon(box, outline=color, width=5)\n",
        "        print(\"\\nğŸ“ íƒì§€ í™•ì¸ (easy.py ìŠ¤íƒ€ì¼): ë¹¨ê°•=ì—¬ëŸ¬ ê¸€ì, ì´ˆë¡=í•œ ê¸€ì\")\n",
        "        plt.figure(figsize=(12, 12))\n",
        "        plt.title(\"Text boxes (PIL polygon)\")\n",
        "        plt.imshow(img_draw)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        del preprocessed_bgr, pil_image_sent, raw_output, text_boxes\n",
        "        if version == \"horizontal\":\n",
        "            del left_pil, right_pil\n",
        "        else:\n",
        "            del pil_image\n",
        "        clean_memory()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì˜¤ë¥˜: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        clean_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69cbe8f7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
