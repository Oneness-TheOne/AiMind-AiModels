import pdfplumber
import json
import os
import glob
import time
from dotenv import load_dotenv

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field
import re
from typing import List, Optional
from tqdm import tqdm

# .env íŒŒì¼ì˜ ë‚´ìš©ì„ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# 1. ë°ì´í„° êµ¬ì¡° ì •ì˜
class HTPIndicator(BaseModel):
    element: str = Field(description="ê·¸ë¦¼ ìš”ì†Œ (ì˜ˆ: ì§‘, ë‚˜ë¬´, ì‚¬ëŒ ë“±)")
    feature: str = Field(description="ê·¸ë¦¼ì˜ êµ¬ì²´ì  íŠ¹ì§• (ì˜ˆ: ì°½ë¬¸ ì—†ìŒ, ì˜¹ì´)")
    interpretation: str = Field(description="ì‹¬ë¦¬ì  ì˜ë¯¸ ë° í•´ì„ (í†µê³„ì  ê·¼ê±° í¬í•¨)")
    source: str = Field(description="ì¶œì²˜ ë…¼ë¬¸ íŒŒì¼ëª…")
    category: str = Field(description="ì§€í‘œ ì„±ê²© (ì •ì„œ, ë°œë‹¬, í˜•ì‹ ë“±)")
    page: Optional[str] = Field(None, description="ì¶œì²˜ í˜ì´ì§€ (ì˜ˆ: 3 ë˜ëŠ” 3-5)")

class HTPDictionary(BaseModel):
    indicators: List[HTPIndicator]

# ì°¸ê³  ë…¼ë¬¸ ì¶œì²˜ í•„ë“œëª… (JSONÂ·ChromaDB ë©”íƒ€ë°ì´í„° í‚¤, HTPIndicator.sourceì™€ ë™ì¼)
SOURCE_FIELD = "source"
PAGE_FIELD = "page"

def _pages_from_chunk(text: str) -> str:
    """ì²­í¬ í…ìŠ¤íŠ¸ì—ì„œ '--- Page N ---' íŒ¨í„´ì„ ì°¾ì•„ í˜ì´ì§€ ë²”ìœ„ ë¬¸ìì—´ ë°˜í™˜ (ì˜ˆ: '3' ë˜ëŠ” '3-5')."""
    nums = re.findall(r"--- Page (\d+) ---", text)
    if not nums:
        return ""
    nums = sorted(set(int(n) for n in nums))
    if len(nums) == 1:
        return str(nums[0])
    return f"{nums[0]}-{nums[-1]}"


def _safe_chunk_filename(source_name: str, index: int) -> str:
    """ì¶œì²˜ íŒŒì¼ëª…ê³¼ ì²­í¬ ì¸ë±ìŠ¤ë¡œ ì €ì¥ìš© íŒŒì¼ëª… ìƒì„± (ê³µë°±Â·íŠ¹ìˆ˜ë¬¸ì ì œê±°)."""
    stem = os.path.splitext(source_name)[0]
    stem = re.sub(r"[^\w\u3130-\u318f\uac00-\ud7af\-]", "_", stem)
    stem = (stem[:50] + "_") if len(stem) > 50 else stem
    return f"{stem}_chunk_{index:03d}.txt"

# 2. PDF í…ìŠ¤íŠ¸ ë° í‘œ ì¶”ì¶œ í•¨ìˆ˜
def extract_pdf_with_tables(pdf_path):
    full_text = ""
    print(f"ğŸ“‚ ë¶„ì„ ì¤‘: {os.path.basename(pdf_path)}")
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text() or ""
            tables = page.extract_tables()
            table_text = ""
            if tables:
                table_text += "\n\n[--- í‘œ ë°ì´í„° ì‹œì‘ ---]\n"
                for table in tables:
                    for row in table:
                        row_str = " | ".join([str(cell).replace('\n', ' ') if cell else "" for cell in row])
                        table_text += f"| {row_str} |\n"
                    table_text += "\n"
                table_text += "[--- í‘œ ë°ì´í„° ë ---]\n\n"
            full_text += f"\n--- Page {i+1} ---\n{text}{table_text}"
    return full_text

# 3. Gemini 2.5 Flash-Lite ë¶„ì„ ë¡œì§
def process_with_gemini(text, source_name, api_key, chunks_dir=None):
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite", 
        google_api_key=api_key,
        temperature=0,
        # ğŸŒŸ í•´ê²°ì±… 1: ì¶œë ¥ í† í° í•œë„ë¥¼ ë†’ê²Œ ì„¤ì • (ìµœëŒ€ 8192 ë“±)
        max_output_tokens=8192 
    )
    parser = PydanticOutputParser(pydantic_object=HTPDictionary)
    
    prompt = PromptTemplate(
        template="""ë‹¹ì‹ ì€ ì•„ë™ ì‹¬ë¦¬ ë¶„ì„ ë° HTP ê²€ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì œê³µëœ ë…¼ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ì‹¬ë¦¬ í•´ì„ ì§€í‘œë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”.

        ### í•µì‹¬ ì§€ì¹¨:
        1. **ì¤‘ë³µ ê¸ˆì§€ (ë§¤ìš° ì¤‘ìš”)**: 
            - ë™ì¼í•œ ë‚´ìš©ì˜ ì§€í‘œë¥¼ ë°˜ë³µí•´ì„œ ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš”. 
            - í•œ ë²ˆì˜ ë‹µë³€ì—ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê³ ìœ í•œ ì§€í‘œë“¤ë§Œ í¬í•¨í•˜ì„¸ìš”.
        2. **ì„¹ì…˜ íŒŒì•…**: 'ì§‘(H)', 'ë‚˜ë¬´(T)', 'ì‚¬ëŒ(P)' êµ¬ë¶„ì„ ì—„ê²©íˆ í•˜ì„¸ìš”.
        3. **ì™„ì „í•œ JSON**: ë°˜ë“œì‹œ ëê¹Œì§€ ì™„ì„±ëœ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë¹ˆ ê°ì²´`{{}}`ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        {format_instructions}

        SOURCE: {source_name}
        TEXT: {text}""",
        input_variables=["text", "source_name"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )

    # ğŸŒŸ í•´ê²°ì±… 2: Tier 1ì´ë¯€ë¡œ ìš”ì²­ íšŸìˆ˜ë³´ë‹¤ëŠ” 'ì¶œë ¥ ì•ˆì •ì„±'ì„ ìœ„í•´ 
    # ì²­í¬ ì‚¬ì´ì¦ˆë¥¼ 15000ì—ì„œ 8000~10000 ì •ë„ë¡œ ì•½ê°„ ë‚®ì¶¥ë‹ˆë‹¤.
    splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
    docs = splitter.split_text(text)
    
    # ì²­í¬ëœ í…ìŠ¤íŠ¸ë¥¼ results/chunks/ ì— ì €ì¥ (chunks_dir ì§€ì • ì‹œ)
    if chunks_dir:
        os.makedirs(chunks_dir, exist_ok=True)
        for i, doc in enumerate(docs):
            path = os.path.join(chunks_dir, _safe_chunk_filename(source_name, i + 1))
            with open(path, "w", encoding="utf-8") as f:
                f.write(doc)
        print(f"  ğŸ“ ì²­í¬ {len(docs)}ê°œ ì €ì¥: {chunks_dir}")
    
    results = []
    pbar = tqdm(docs, desc=f"ğŸ” ë¶„ì„ ì¤‘: {source_name[:15]}...", leave=False)
    
    for i, doc in enumerate(pbar):
        max_retries = 3
        success = False
        retry_count = 0
        
        while retry_count < max_retries and not success:
            try:
                pbar.set_postfix(section=f"{i+1}/{len(docs)}", status="working")
                output = llm.invoke(prompt.format(text=doc, source_name=source_name))
                
                # ğŸŒŸ í•´ê²°ì±… 3: ëª¨ë¸ì˜ ì‘ë‹µì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                if not output.content.strip():
                    retry_count += 1
                    continue

                parsed = parser.parse(output.content)
                page_str = _pages_from_chunk(doc)
                # ë¹ˆ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° í•„í„°ë§ + ì¶œì²˜ í˜ì´ì§€ ë¶€ì—¬
                valid_indicators = [ind for ind in parsed.indicators if ind.element]
                for ind in valid_indicators:
                    ind.page = page_str or None
                results.extend(valid_indicators)
                
                success = True
                time.sleep(2) # Tier 1ì´ë¯€ë¡œ ëŒ€ê¸° ì‹œê°„ì„ 2ì´ˆë¡œ ì¤„ì—¬ë„ ë©ë‹ˆë‹¤.
                
            except Exception as e:
                retry_count += 1
                pbar.set_postfix(retry=retry_count, error="Parsing...")
                time.sleep(5)
                    
    return results

def run_pdf_to_json(thesis_dir="thesis", result_dir="results", api_key=None):
    """
    thesis_dir ë‚´ PDFë¥¼ ì²­í‚¹Â·ì§€í‘œ ì¶”ì¶œ í›„ result_dirì— JSON ì €ì¥.
    Returns: ì €ì¥ëœ JSON íŒŒì¼ ê²½ë¡œ. PDF ì—†ìœ¼ë©´ None.
    """
    from gemini_integration import resolve_gemini_api_key
    api_key = resolve_gemini_api_key(api_key)
    if not api_key:
        print("âŒ GEMINI_API_KEY ë˜ëŠ” GEMINI_API_KEYSê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return None
    pdf_files = glob.glob(os.path.join(thesis_dir, "*.pdf"))
    if not pdf_files:
        print(f"âŒ {thesis_dir} í´ë”ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    chunks_dir = os.path.join(result_dir, "chunks")
    final_data = []
    for file_path in pdf_files:
        content = extract_pdf_with_tables(file_path)
        data = process_with_gemini(
            content,
            os.path.basename(file_path),
            api_key,
            chunks_dir=chunks_dir,
        )
        final_data.extend([d.dict() for d in data])
    os.makedirs(result_dir, exist_ok=True)
    base_name = "htp_final_dataset"
    extension = ".json"
    output_filename = os.path.join(result_dir, f"{base_name}{extension}")
    counter = 2
    while os.path.exists(output_filename):
        output_filename = os.path.join(result_dir, f"{base_name}_{counter}{extension}")
        counter += 1
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
    print(f"\nğŸ‰ ì™„ë£Œ! ì´ {len(final_data)}ê°œ ì§€í‘œê°€ '{output_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return output_filename


if __name__ == "__main__":
    out = run_pdf_to_json(thesis_dir="./thesis", result_dir="results")
    if out:
        print(f"ë‹¤ìŒìœ¼ë¡œ ë²¡í„° DB ì ì¬: python main.py ingest --json {out}")